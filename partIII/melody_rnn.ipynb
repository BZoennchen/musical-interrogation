{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/BZoennchen/musical-interrogation/blob/main/partIII/melody_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgenden 6 Zellen sind für die Ausführung im Colab nötig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title clone git repository\n",
    "%%capture\n",
    "!rm -rf musical-interrogation\n",
    "!git clone https://github.com/BZoennchen/musical-interrogation.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title move into directory\n",
    "%%capture\n",
    "import zipfile\n",
    "import os\n",
    "os.chdir('musical-interrogation/partIII')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title install dependencies to play sound\n",
    "%%capture\n",
    "print('installing fluidsynth...')\n",
    "!apt-get install fluidsynth > /dev/null\n",
    "!cp /usr/share/sounds/sf2/FluidR3_GM.sf2 ./font.sf2\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title install dependencies to show score in music notation\n",
    "%%capture\n",
    "print('installing musescore3...')\n",
    "!apt-get install musescore3 > /dev/null\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title install python libs\n",
    "%%capture\n",
    "!pip install torch torchview music21 matplotlib fluidsynth midi2audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title unzip dataset\n",
    "%%capture\n",
    "with zipfile.ZipFile('./../data/erk.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./../deutschl/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN) / LSTM\n",
    "\n",
    "*RNNs* erlauben es uns auf Grundlage einer belieb langen endlichen Sequenz, das nächste Element der Sequenz vorherzusagen. Wie gut diese Vorhersage ist, ist eine andere Frage.\n",
    "\n",
    "Anstatt lediglich direkte Übergänge zu berücksichtigen wollen wir Sequenzen der Länge ``sequence_len`` beim Training betrachten. Das bedeutet wir müssen bei der Datenvorbereitung beachten, dass wir derartige Sequenzen aus den Daten konstruieren. Zwar kann unser trainiertes RNN auch längere Sequenzen generieren, allerdings hat es derartige Sequenzen nie zuvor gesehen. Deshalb wird die Qualität abnehmen.\n",
    "\n",
    "Desweiteren werden wir diesmal anstatt Noten, Events die alle die gleiche Dauer haben (einen Zeitschritt) betrachten.\n",
    "Eine Note wird durch ein **X-NoteOn**-Event und der darauffolgenden **NoteHold**-Event repräsentiert. Z.B.\n",
    "\n",
    "``65 _ _ _ 77 _ r _ _ ``\n",
    "\n",
    "Würde bedeuten, dass das Stück mit der MIDI-Note 65 (**65-NoteOn**-Event) beginnt.\n",
    "Die Note wird über 3 weitere Zeitschritte gehalten (also insgesamt 4), dann folgt ein **77-NoteOn**-Event, welches über einen weiteren Zeitschritt gehalten wird und schließlich folgt eine Pause von insgesamt 3 Zeitschritten.\n",
    "\n",
    "Anstatt des ``NoteEncoder`` verwenden wir deshalb den ``GridEncoder``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\") \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchview import draw_graph\n",
    "\n",
    "import music21 as m21\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from preprocess import load_songs_in_kern, GridEncoder, StringToIntEncoder\n",
    "from preprocess import TERM_SYMBOL, TIME_STEP\n",
    "from dataset import ScoreDataset\n",
    "\n",
    "from utils import score_to_wav\n",
    "from IPython.display import Audio\n",
    "\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Datenvorbereitung\n",
    "\n",
    "Zunächst laden wir unsere (1700 **einstimmigen**) Musikstücke aus denen wir die Frequenzen berechnen wollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes a while\n",
    "scores = load_songs_in_kern('./../deutschl/erk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können uns eines der Musikstücke anhören oder auch anzeigen lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(score_to_wav(scores[0], 'score1.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes verwandeln wir die Noten in gut leserliche Zeichenketten, wobei jedes Event durch genau eine Zeichenkette repräsentiert wird (siehe oben).\n",
    "\n",
    "Dies übernimmt der ``GridEncoder``. Dieser transponiert die Musikstücke zusätzlich nach C-Dur.\n",
    "Dieser filtert zugleich Musikstücke heraus, welche wir mit unserem ``time_step`` nicht abbilden können.\n",
    "Z.B. wenn ``time_step = 1/8`` dann können wir keine ``1/16``-Noten oder auch ``1/8 + 1/16``-Noten abbilden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes a while\n",
    "time_step = 1/16\n",
    "print(f'one timestep represents {time_step} beats')\n",
    "\n",
    "encoder = GridEncoder(time_step)\n",
    "enc_songs, invalid_song_indices = encoder.encode_songs(scores)\n",
    "\n",
    "print(f'there are {len(enc_songs)} valid songs and {len(invalid_song_indices)} songs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beispielsweise lässt sich mit ``time_step = 1/16`` folgendes Musikstück nicht abbilden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores[invalid_song_indices[0]].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können ein Musikstück in der codierten Form ausgeben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(enc_songs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f'longest melody: {max(len(m) for m in enc_songs)}')\n",
    "print(f'shortest melody: {min(len(m) for m in enc_songs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da der Computer besser mit Zahlen umgehen kann bauen wir uns eine Abbildung von den jeweiligen Zeichenketten zu Zahlen $$\\{0, 1, 2, \\ldots, m-1\\}$$ und umgekehrt. Dies übernimmt ``StringToIntEncoder``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = StringToIntEncoder(enc_songs)\n",
    "print(f'number of unique symbols: {len(string_to_int)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_symbol = string_to_int.encode(enc_songs[0][0])\n",
    "print(f'midi-ptich {enc_songs[0][0]} is encoded to number {encoded_symbol}')\n",
    "print(f'number {encoded_symbol} is decoded to midi-pitch {string_to_int.decode(encoded_symbol)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Konstruktion der Trainingsdaten\n",
    "\n",
    "``ScoreDataset`` verwaltet unsere Daten und lässt uns, in Kombination mit einem ``DataLoader``, bequem Sequenzen (d.h. Teile eines Stücks) der Länge ``sequence_len`` (Zeitschritte) laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_len = 64 # this is a hyperparameter!\n",
    "dataset = ScoreDataset(enc_songs=enc_songs, stoi_encoder=string_to_int, sequence_len=sequence_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``sequence_len * time_step`` ergibt die Anzahl der Beats die wir beim Lernen betrachten (ist im Fall einer 4/4 Signatur ``sequence_len * (time_step/0.25)``)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'while training we are looking at {sequence_len * (time_step/0.25)} beats')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir teilen die Daten nun in Trainings-, Validierungs-, und Testdaten auf.\n",
    "\n",
    "+ Trainingsdaten: Verwenden wir zum Training unseres Modells / Melodiegenerators\n",
    "+ Validierungsdaten: Verwenden wir um unseren Lernerfolg während des Trainings zu vergleichen\n",
    "+ Testdaten: Verwenden wir am Ende des Trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modelldefinition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im folgenden sehen wir alle wichtigen sog. ``Hyperparameter``, d.h. Parameter die wir eventuell noch anpassen wollen um ein besseres Ergebnis zu erhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### start hyperparameters #####\n",
    "vocab_size = len(string_to_int) # size of our alphabet\n",
    "input_dim = vocab_size # can be different\n",
    "hidden_dim = 128 # can be different\n",
    "layer_dim = 1 # can be different\n",
    "output_dim = vocab_size # should not be different\n",
    "dropout = 0.2 # can be different\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.001 # can be different\n",
    "batch_size = 64 # can be different\n",
    "n_epochs = 10 # can be different\n",
    "eval_interval = 100 # can be different\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    #torch.backends.mps.empty_cache()\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "##### end hyperparameters #####\n",
    "\n",
    "print(f'{device=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es folgt die Modellbeschreibung unseres RNNs/LSTMs. \n",
    "Um zu verstehen was vorsich geht, blicken Sie auf die Methode ``forward``.\n",
    "Diese schickt unsere Daten durch das Netz.\n",
    "\n",
    "Die ersten beiden Zeilen erstellen die Kurz- und Langzeitspeicher und füllen diese mit lauter nullen.\n",
    "\n",
    "Dann findet ein sog. Embedding statt: ``x = self.embedding(x)``. Dies ist nichts anderes als das was wir mit unserem einfachen *Feedforward Net* gemacht haben: Jedes Element der Eingabe ``x`` wird erst *one-hot* encoded und dann an eine Matrix multipliziert. Das Resultat: Jedes Event wird durch die Zeile einer Matrix repräsentiert. Die Matrix besitzt ``vocab_size`` Zeilen und ``input_dim`` Spalten.\n",
    "\n",
    "Als nächstes schicken wir unsere umgewandelte Eingabe durch unsere LSTM ``out, (ht, ct) = self.lstm(x, (h0, c0))``.\n",
    "Wir erhalten so viele Ausgaben wie unsere Sequenz lang ist, d.h. ``sequence_len`` viele.\n",
    "Wir interessieren uns aber nur für die letzte Ausgabe, die wir uns mit ``out[:, -1, :]`` holen.\n",
    "Dies ist ein Vektor mit ``hidden_dim`` Elementen. ``ht`` und ``ct`` brauchen wir nicht.\n",
    "\n",
    "Dann schicken wir diese durch eine Dropout Schicht um der Überanpassung entgegenzuwirken.\n",
    "\n",
    "Im letzten Schritt transformieren wir den ``hidden_dim``-dimensionalen Vektor in einen ``output_dim``-dimensionalen Vektor, was wirderum gleich ``vocab_size`` ist.\n",
    "\n",
    "Dieser Vektor wird als Wahrscheinlichkeitsverteilung interpretiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(vocab_size, input_dim)\n",
    "        self.lstm = torch.nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim, device=device)\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim, device=device)\n",
    "        \n",
    "        # x = B, T, C\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        out, (ht, ct) = self.lstm(x, (h0, c0))\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out # B, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim, dropout)\n",
    "model.to(device)  # use gpu if possible\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgende Zelle dient lediglich der Visualisierung unseres Modells und hat keine Auswirkung auf die Berechnung.\n",
    "Wir holen uns einen **Batch** von Sequenzen (der Länge ``sequenz_len``)  und das jeweils zugehörige **Label** (das auf die Sequenz folgende Event).\n",
    "\n",
    "``X_vis`` ist ein **Tensor** mit **Shape** \n",
    "\n",
    "$$\\text{batch_size} \\times \\text{sequence_length}$$\n",
    "\n",
    "geschrieben als ``(batch_size, sequence_length)``, hier ``(64, 32)``. ``X_vis`` jagen wir durch unser Netzwerk/Modell um es anschließend Anzeigen zu können.\n",
    "\n",
    "Die Visualisierung ist nützlich um zu sehen wie der Eingabe-**Tensor** verändert wird.\n",
    "Dabei sind **Batches** Anfangs etwas verwirrend.\n",
    "Wir können uns auch die erste Dimension, d.h., ``batch_size`` \"wegdenken\".\n",
    "Das ``Embedding`` transformiert unsere Sequenz in \n",
    "\n",
    "$$\\text{sequence_length} \\times \\text{input_dim}$$\n",
    "\n",
    "Hier findet impliziet ein *one-hot* Encoding statt in Kombination mit einer Matrixmultiplikation, d.h., genau das was wir in unserem **Feedforward Net** gemacht hatten.\n",
    "Jedes Symbol wird durch einen Vektor der länge \"Anzahl an unterschiedlicher Symbole\" repräsentiert.\n",
    "\n",
    "Anschließend wird der **Tensor** durch das eigentliche LSTM gejagt.\n",
    "Neben diesem Tensor erwartet das LSTM noch einen Initialtensor für die **Hiddensates** $h_0$ und $c_0$.\n",
    "Das Resultat ist ein neuer Output-Tensor:\n",
    "\n",
    "$$\\text{sequence_length} \\times \\text{hidden_dim}$$\n",
    "\n",
    "und die beiden letzten **Hiddenstates** $h_{\\text{k}-1}$, $c_{\\text{k}-1}$ mit $k$ = ``sequence_length`` die jeweils ``hidden_dim`` Zahlen enthalten.\n",
    "\n",
    "Da wir nur am letzten Output interessiert sind, d.h., genau das Symbol was uns das LSTM vorhersagt, holen wir uns mit ``__getitem__`` dieses Symbol codiert durch ``hidden_dim`` Zahlen.\n",
    "\n",
    "``Dropout`` dient dem Entgegenwirken der **Überanpassung**. Dabei werden beim Training zufällig bestimmte künstliche Neuronen deaktiviert.\n",
    "\n",
    "Man müsste denken, dass wir im letzten Schritt aus den ``hidden_dim`` Zahlen das entsprechende Symbol, also eine Zahl, erzeugen müssen. Was jedoch nützlicher ist, ist es eine Wahrscheinlichkeitsverteilung zu erhalten, d.h., einen Vektor mit ``vocab_size``Zahlen wobei jeder Zahl die Wahrscheinlichkeit für eben jenes Symbol widerspiegelt.\n",
    "Genau das passiert!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (batch_size, sequence_len)\n",
    "X_vis, y_vis = train_set[0:batch_size]\n",
    "print(f'shape of X_vis: {X_vis.shape}')\n",
    "print(f'shape of y_vis: {y_vis.shape}')\n",
    "print(f'number of different symbols {vocab_size}')\n",
    "X_vis, y_vis = X_vis.to(device), y_vis.to(device)\n",
    "model_vis = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim, dropout)\n",
    "model_graph = draw_graph(model_vis, input_data=X_vis, device=device)\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Melodiegenerierung vor dem Training\n",
    "\n",
    "Gegeben einer Sequenz beliebiger Länge, dient die Funktion ``generate`` der Generierung eines neues neuen Musikstücks.\n",
    "``temperature`` bestimmt wie stark die vom Modell gelernte Wahrscheinlichkeitsverteilung beachtet wird.\n",
    "\n",
    "+ ``temperature`` gleich 1.0 bedeutet, dass von der Wahrscheinlichkeitsverteilung gesampelt wird.\n",
    "+ ``temperature`` gegen unendlich bedeutet, dass gleichverteilt gesampelt wird (mehr Variation)\n",
    "+ ``temperature`` gegen 0 bedeutet, dass die hohe Wahrscheinlichkeiten verstärkt werden (weniger Variation)\n",
    "\n",
    "Sie können eine maximale Länge des Stücks festlegen und auch einen Anfang eines Stücks mitliefern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_event_number(idx, temperature:float):\n",
    "    with torch.no_grad():\n",
    "        logits = model(idx)\n",
    "        probs = F.softmax(logits / temperature, dim=1) # B, C\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        return idx_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(seq: list[str]=None, max_len:int=None, temperature:float=1.0):\n",
    "    with torch.no_grad():\n",
    "        generated_encoded_song = []\n",
    "        if seq != None:\n",
    "            idx = torch.tensor([[string_to_int.encode(char) for char in seq]], device=device)\n",
    "            generated_encoded_song = seq.copy()\n",
    "        else:\n",
    "            idx = torch.tensor([[string_to_int.encode(TERM_SYMBOL)]], device=device)\n",
    "        \n",
    "        while max_len == None or max_len > len(generated_encoded_song):\n",
    "            idx_next = next_event_number(idx, temperature)\n",
    "            char = string_to_int.decode(idx_next.item())\n",
    "            if idx_next == string_to_int.encode(TERM_SYMBOL):\n",
    "                break\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # B, T+1, C\n",
    "            generated_encoded_song.append(char)\n",
    "            \n",
    "        return generated_encoded_song"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wir neue Musikstücke generieren bevor das RNN trainiert wurde, dann erhalten wir keine guten Ergebnisse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of songs we want to generate\n",
    "n_scores = 5\n",
    "temperature = 0.6\n",
    "before_new_songs = []\n",
    "for _ in range(n_scores):\n",
    "    encoded_song = generate(max_len=13,temperature=temperature)\n",
    "    print(f'generated {\" \".join(encoded_song)} conisting of {len(encoded_song)} notes')\n",
    "    before_new_songs.append(encoded_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_generated_scores = encoder.decode_songs(before_new_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "before_generated_scores[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(score_to_wav(before_generated_scores[2], 'before_g_song.wav'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training\n",
    "\n",
    "Zum Training verwenden wir hier einen sog. ``DataLoader``. Dieser hilft uns dabei auf unsere Daten einfacher zugreifen zu können. Z.B., lassen wir unsere Daten vor dem Training durchmischen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'there are {len(train_set)} data points for training')\n",
    "print(f'there are {len(val_set)} data points for validation')\n",
    "print(f'there are {len(test_set)} data points for testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Code für das Training wirkt etwas kompliziert, da wir **Batches** verwenden, da wir es mit sehr vielen Daten zu tun haben und diese nicht immer alle (pro Trainingsschritt) durchs Netz schicken sondern immer nur einen Teil, nämlich ``batch_size`` viele. Eine **Epoche** ist dadruch definiert, dass in ihr alle Trainingsdaten einmal durchs Netz geschickt wurden.\n",
    "\n",
    "Im Wesentlichen geschieht nichts anderes als:\n",
    "\n",
    "1. Schicke **Batch** durchs netz (Forwardpass)\n",
    "2. Berechne Fehler/Kosten\n",
    "3. Propagiere Gradienten der Kostenfunktion bzgl. der **Modellparameter** rückwärts durchs Netz (Backwardpass)\n",
    "4. Update Modellparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer, n_epochs):\n",
    "    running_loss = 0.0\n",
    "    last_loss = 0.0\n",
    "    all_steps = n_epochs * len(train_loader)\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        local_X, local_y = data\n",
    "        local_X, local_y = local_X.to(device), local_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(local_X)\n",
    "        \n",
    "        loss = criterion(outputs, local_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % eval_interval == eval_interval-1:\n",
    "            last_loss = running_loss / eval_interval\n",
    "            \n",
    "            steps = epoch_index * len(train_loader) + (i+1)\n",
    "            \n",
    "            print(\n",
    "                f'Epoch [{epoch_index+1}/{n_epochs}], Step [{steps}/{all_steps}], Loss: {last_loss:.4f}')\n",
    "            tb_x = epoch_index * len(train_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "            \n",
    "    return last_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "def train(n_epochs,respect_val=False):\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "    best_vloss = 1_000_000\n",
    "\n",
    "    for epoch in range(n_epochs):    \n",
    "        model.train(True)\n",
    "        avg_loss = train_one_epoch(epoch, writer, n_epochs)\n",
    "        \n",
    "        model.train(False)\n",
    "        running_vloss = 0.0\n",
    "        \n",
    "        for i, vdata in enumerate(val_loader):\n",
    "            \n",
    "            local_X, local_y = vdata\n",
    "            local_X, local_y = local_X.to(device), local_y.to(device)\n",
    "            \n",
    "            voutputs = model(local_X)\n",
    "            vloss = criterion(voutputs, local_y)\n",
    "            running_vloss += vloss\n",
    "            \n",
    "        avg_vloss = running_vloss / (i+1)\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}], Train-Loss: {avg_loss:.4f}, Val-Loss: {avg_vloss:.4f}')\n",
    "        \n",
    "        writer.add_scalars('Training vs. Validation Loss', {'Training': avg_loss, 'Validation': avg_vloss}, epoch)\n",
    "        writer.flush()\n",
    "        \n",
    "        if not respect_val or (respect_val and avg_vloss < best_vloss):\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = './models/_model_{}_{}'.format(timestamp, epoch)\n",
    "            torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'there are the following models to choose from:')\n",
    "\n",
    "for model_file in os.listdir('./models/'):\n",
    "    print(f'./models/{model_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das beste Modell des Trainings findet ihr im Ordner ``musical-interrogation/rnn/`` und könnt ihr wie folgt laden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads a saved model\n",
    "model_path = './models/pretrained_1_128_best_val'\n",
    "\n",
    "if device.type == 'cpu':\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "elif torch.backends.mps.is_available():\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('mps')))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Melodiegenerierung nach dem Training\n",
    "\n",
    "Nach dem Training sollten die generierten Stücke eine bessere Qualität besitzen. Das bedeutet jedoch lediglich, dass die *Likelihood* besser ist und sagt noch nichts darüber aus ob die Stücke auch musikalisch \"besser\" sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of songs we want to generate\n",
    "n_scores = 5\n",
    "temperature = 0.6\n",
    "after_new_songs = []\n",
    "for _ in range(n_scores):\n",
    "    encoded_song = generate(max_len=120,temperature=temperature)\n",
    "    print(f'generated {\" \".join(encoded_song)} conisting of {len(encoded_song)} notes')\n",
    "    after_new_songs.append(encoded_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_generated_scores = encoder.decode_songs(after_new_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "after_generated_scores[2].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(score_to_wav(after_generated_scores[0], 'a_g_song.wav'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können auch einen Teil bestehendes Musikstücks verwenden und diesen erweitern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(enc_songs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_notes = 10\n",
    "part = encoder.take_notes(enc_songs[0], n_notes)\n",
    "' '.join(part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(score_to_wav(encoder.decode_song(part), 'part.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_song = generate(part, max_len=120)\n",
    "' '.join(enc_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song = encoder.decode_song(enc_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(score_to_wav(song, 'g_song.wav'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fragen\n",
    "\n",
    "+ Welches Problem fangen wir uns ein wenn wir Noten mit einer anderen Dauer unterstützen wollen?\n",
    "+ Könnten wir eventuell nicht unterstützte Notenlängen einfach umwandeln?\n",
    "+ Wie erweitern wir unser System sodass wir Vielstimmigkeit lernen und generieren können und was würde das für das Training bedeuten?\n",
    "+ Was bedeutet *Überanpassung* (Overfitting) im Kontext unseres Problems? Kann diese wünschenswert sein?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
