{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/BZoennchen/musical-interrogation/blob/main/partIV/melody-transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgenden 5 Zellen sind für die Ausführung im Colab nötig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title clone git repository\n",
    "%%capture\n",
    "!rm -rf musical-interrogation\n",
    "!git clone https://github.com/BZoennchen/musical-interrogation.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title move into directory\n",
    "%%capture\n",
    "import zipfile\n",
    "import os\n",
    "os.chdir('musical-interrogation/partIV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title install dependencies to play sound\n",
    "%%capture\n",
    "print('installing fluidsynth...')\n",
    "!apt-get install fluidsynth > /dev/null\n",
    "!cp /usr/share/sounds/sf2/FluidR3_GM.sf2 ./font.sf2\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title install dependencies to show score in music notation\n",
    "%%capture\n",
    "print('installing musescore3...')\n",
    "!apt-get install musescore3 > /dev/null\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title install python libs\n",
    "%%capture\n",
    "!pip install torch torchview music21 matplotlib fluidsynth midi2audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "**AICA Crashkurs, Dr. Benedikt Zönnchen**\n",
    "\n",
    "Auch wenn das Modell in diesem Notebook komplizierter scheint als unser LSTM, der Kern des Transformers -- der Attention-Mechanismus -- ist in der Klasse ``Head`` implementiert.\n",
    "Alles drum herum dient der Optimierung (Vermeidung von Überanpassung und \"aufblasen\" der Netzwerkkomplexität)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "# Entpacke die zip-Datei, welche die Trainingsdaten enthält in den richtigen Ordner.\n",
    "with zipfile.ZipFile('./../data/erk.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./../deutschl/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\") \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchview import draw_graph\n",
    "\n",
    "import music21 as m21\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from preprocess import load_songs_in_kern, GridEncoder, StringToIntEncoder\n",
    "from preprocess import TERM_SYMBOL, TIME_STEP\n",
    "from dataset import ScoreDataset\n",
    "\n",
    "from utils import score_to_wav\n",
    "from IPython.display import Audio\n",
    "\n",
    "import graphviz\n",
    "import numpy as np\n",
    "\n",
    "graphviz.set_jupyter_format('png')\n",
    "\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes a while!!!\n",
    "# load ker files and transform them into m21.Scores\n",
    "scores = load_songs_in_kern('./../deutschl/erk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(score_to_wav(scores[0], 'score1.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes verwandeln wir die Noten in gut leserliche Zeichenketten, wobei jedes Event durch genau eine Zeichenkette repräsentiert wird.\n",
    "\n",
    "Dies übernimmt der ``GridEncoder``. Dieser transponiert die Musikstücke zusätzlich nach C-Dur.\n",
    "Dieser filtert zugleich Musikstücke heraus, welche wir mit unserem ``time_step`` nicht abbilden können.\n",
    "Z.B. wenn ``time_step = 1/8`` dann können wir keine ``1/16``-Noten oder auch ``1/8 + 1/16``-Noten abbilden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes a while\n",
    "time_step = 1/16\n",
    "print(f'one timestep represents {time_step} beats')\n",
    "\n",
    "encoder = GridEncoder(time_step)\n",
    "enc_songs, invalid_song_indices = encoder.encode_songs(scores)\n",
    "\n",
    "print(f'there are {len(enc_songs)} valid songs and {len(invalid_song_indices)} songs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores[invalid_song_indices[0]].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können ein Musikstück in der codierten Form ausgeben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(enc_songs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'longest melody: {max(len(m) for m in enc_songs)}')\n",
    "print(f'shortest melody: {min(len(m) for m in enc_songs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da der Computer besser mit Zahlen umgehen kann bauen wir uns eine Abbildung von den jeweiligen Zeichenketten zu Zahlen $$\\{0, 1, 2, \\ldots, m-1\\}$$ und umgekehrt. Dies übernimmt ``StringToIntEncoder``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = StringToIntEncoder(enc_songs)\n",
    "print(f'number of unique symbols: {len(string_to_int)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_symbol = string_to_int.encode(enc_songs[0][0])\n",
    "print(f'midi-ptich {enc_songs[0][0]} is encoded to number {encoded_symbol}')\n",
    "print(f'number {encoded_symbol} is decoded to midi-pitch {string_to_int.decode(encoded_symbol)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Konstruktion der Trainingsdaten\n",
    "\n",
    "``ScoreDataset`` verwaltet unsere Daten und lässt uns in Kombination über einen ``DataLoader`` bequem Sequenzen (d.h. Teile eines Stücks) der Länge ``sequence_len`` (Zeitschritte) laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_len = 64 # this is a hyperparameter!\n",
    "dataset = ScoreDataset(enc_songs=enc_songs, stoi_encoder=string_to_int, sequence_len=sequence_len, in_between=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``sequence_len * time_step`` ergibt die Zeit (bzw. ist im Fall einer 4/4 Signatur ``sequence_len * (time_step/0.25)`` die Anzahl der Beats die wir beim Lernen betrachten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'while training we are looking at {sequence_len * (time_step/0.25)} beats')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir teilen die Daten nun in Trainings-, Validierungs-, und Testdaten auf.\n",
    "\n",
    "+ Trainingsdaten: Verwenden wir zum Training unseres Modells / Melodiegenerators\n",
    "+ Validierungsdaten: Verwenden wir um unseren Lernerfolg während des Trainings zu vergleichen\n",
    "+ Testdaten: Verwenden wir am Ende des Trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelldefinition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### start hyperparameters #####\n",
    "batch_size = 64\n",
    "n_embd = 32 # has to be devisible by n_heads\n",
    "n_heads = 2\n",
    "n_blocks = 2\n",
    "dropout = 0.2\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "vocab_size = len(string_to_int)\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_epochs = 10\n",
    "eval_interval = 100\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    #torch.backends.mps.empty_cache()\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "##### end hyperparameters #####\n",
    "\n",
    "#device = 'cpu'\n",
    "print(f'{device=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lassen Sie uns die Klasse ``Head`` besprechen, denn diese implementiert den sog. **Attention-Head** des Transformers, d.h. seinen Kern.\n",
    "Der ``Head`` besteht aus ``nn.Linear``-Layern einem Buffer und einem ``nn.Dropout``-Layer.\n",
    "``nn.Linear`` realisiert eine einfache lineare Transformation:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{x}^\\top \\mathbf{W} + \\mathbf{b}$$\n",
    "\n",
    "wobei in unserem Fall $\\mathbf{b} = \\mathbf{0}$ ist, da ``bias=False`` gilt.\n",
    "Es bleibt also $\\mathbf{y} = \\mathbf{x}^\\top \\mathbf{W}$.\n",
    "\n",
    "Wir haben drei solcher Transformationen und jede Transformiert $\\mathbf{X}$ (alle Elemente einer Sequenz) von einem ``n_embd``-dimensionalen Raum in einen ``head_size``-dimensionalen Raum. D.h.\n",
    "\n",
    "$$\\mathbf{K} = \\mathbf{X} \\mathbf{K}$$\n",
    "\n",
    "$$\\mathbf{Q} = \\mathbf{X} \\mathbf{Q}$$\n",
    "\n",
    "$$\\mathbf{V} = \\mathbf{X} \\mathbf{V}$$\n",
    "\n",
    "$\\mathbf{K}$ sind die sog. **Keys**, $\\mathbf{Q}$ die sog. **Querrys**, und $\\mathbf{V}$ die sog. **Values**.\n",
    "Beachten Sie, dass diese Matrizen Werte für alle Elemente einer Sequenz enthalten.\n",
    "Z.B. enthält $\\mathbf{K}$ alle Keys der Elemente einer Sequenz der länge ``sequence_len``.\n",
    "\n",
    "Was wir berechnen wollen, ist die **Attention** die jedes Element in einer Sequenz zu jedem anderen Element spendet.\n",
    "Dabei stellt jedes Element eine Anfrage (ein Querry) und sucht damit nach einem passenden Schlüssel.\n",
    "Je besser Schlüssel und Querry zusammenpassen desto größer ist deren Produkt\n",
    "\n",
    "$$\\mathbf{W} = \\mathbf{Q}\\mathbf{K}^\\top.$$\n",
    "\n",
    "Da $\\mathbf{Q}$ und $\\mathbf{K}$ Matrizen sind, ergibt ihr Produkt eine Matrix.\n",
    "Deren Einträge bestehen aus den ganzen Skalarprodukten der einzelnen Keys und Querrys.\n",
    "Die Zeilen dieser Matrix $\\mathbf{W}$ werden durch *softmax* zu Wahrscheinlichkeitsverteilungen.\n",
    "\n",
    "Am Ende multiplizieren wir $\\mathbf{W} \\mathbf{V}$. Die $i$-te Zeile $\\mathbf{w}_i$ in $\\mathbf{W}$ gewichtet die Werte in $\\mathbf{V}$ für das $i$-te Element.\n",
    "\n",
    "$$\\mathbf{w}_i \\mathbf{V}$$\n",
    "\n",
    "ist das gewichtete Mittel aller Sequenzwerte für das $i$-Element der Sequenz.\n",
    "\n",
    "Da wir nicht in die Zukunft sehen können, maskieren wir das Gewicht für die **Attention** von $i$ auf $j$ sofern $j > i$. Diese Gewichte setzten wir auf 0.\n",
    "Das wird druch die Zeile\n",
    "\n",
    "```\n",
    "wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))\n",
    "```\n",
    "\n",
    "bewirkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size, sequence_len, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(sequence_len, sequence_len)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) # B, T, head_size\n",
    "        q = self.query(x) # B, T, head_size\n",
    "        _, _, head_size = q.shape #???\n",
    "        \n",
    "        wei = q @ k.transpose(-2, -1) * (head_size ** (-0.5)) # B, T, head_size @ B, head_size, T => B, T, T\n",
    "        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        v = self.value(x) # B, T, head_size\n",
    "        out = wei @ v # T, T @ B, T, head_size => B, T, head_size\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_size, sequence_len, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, sequence_len, dropout) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_heads * head_size, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_embd, n_heads, sequence_len, dropout):\n",
    "        super().__init__()\n",
    "        # this could be different\n",
    "        head_size = n_embd // n_heads\n",
    "        self.sa = MultiHeadAttention(n_heads, head_size, sequence_len, dropout)\n",
    "        self.ffwd = FeedForward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) # residual connection\n",
    "        x = x + self.ffwd(self.ln2(x)) # residual connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, sequence_len, n_embd, n_heads, n_blocks, dropout):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(sequence_len, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_heads, sequence_len, dropout) for _ in range(n_blocks)])\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        token_emb = self.token_embedding_table(idx) # B, T, n_embd\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # T, n_embd        \n",
    "        x = token_emb + pos_emb # B, T, n_embd + T, n_embd => B, T, n_embd\n",
    "        x = self.blocks(x) # B, T, head_size\n",
    "        logits = self.lm_head(x) # B, T, vocab_size\n",
    "        return logits\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx = B, T\n",
    "        count = 0\n",
    "        with torch.no_grad():\n",
    "            while count < max_new_tokens:\n",
    "                idx_crop = idx[:, -block_size:]\n",
    "                logits, loss = self(idx_crop) # B, T, C\n",
    "                probs = F.softmax(logits[:,-1,:], dim=1) # B, C\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "                if idx_next == stoi_encoder.encode(TERM_SYMBOL):\n",
    "                    break\n",
    "                idx = torch.cat((idx, idx_next), dim=1)\n",
    "                count += 1\n",
    "            \n",
    "            return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerDecoder(vocab_size, sequence_len, n_embd, n_heads, n_blocks, dropout)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgende Zelle dient lediglich der Visualisierung unseres Modells und hat keine Auswirkung auf die Berechnung.\n",
    "Die Komplexität ist recht hoch da unser Modell aus mehrere Blöcke mit mehreren ``Head``s besteht. Das Training ist dementsprechend aufwendig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# (batch_size, sequence_len)\n",
    "X_vis, y_vis = train_set[0:batch_size]\n",
    "print(f'shape of X_vis: {X_vis.shape}')\n",
    "print(f'shape of y_vis: {y_vis.shape}')\n",
    "print(f'number of different symbols {vocab_size}')\n",
    "X_vis, y_vis = X_vis.to(device), y_vis.to(device)\n",
    "model_vis = TransformerDecoder(vocab_size, sequence_len, n_embd, n_heads, n_blocks, dropout)\n",
    "model_graph = draw_graph(model_vis, input_data=X_vis, device=device)\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training\n",
    "\n",
    "Zum Training verwenden wir hier einen sog. ``DataLoader``. Dieser hilft uns dabei auf unsere Daten einfacher zugreifen zu können. Z.B., lassen wir unsere Daten vor dem Training durchmischen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer, n_epochs):\n",
    "    running_loss = 0.0\n",
    "    last_loss = 0.0\n",
    "    all_steps = n_epochs * len(train_loader)\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        local_X, local_y = data\n",
    "        local_X, local_y = local_X.to(device), local_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(local_X)\n",
    "        \n",
    "        #print(local_X.shape, local_y.shape)\n",
    "        \n",
    "        B, T, C = outputs.shape\n",
    "        outputs = outputs.view(B*T, C)\n",
    "        local_y = local_y.view(B*T)\n",
    "        loss = criterion(outputs, local_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % eval_interval == eval_interval-1:\n",
    "            last_loss = running_loss / eval_interval  # loss per batch\n",
    "            \n",
    "            steps = epoch_index * len(train_loader) + (i+1)\n",
    "            \n",
    "            print(\n",
    "                f'Epoch [{epoch_index+1}/{n_epochs}], Step [{steps}/{all_steps}], Loss: {last_loss:.4f}')\n",
    "            tb_x = epoch_index * len(train_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "            \n",
    "    return last_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "def train(n_epochs, respect_val=False, val_losses=[], train_losses=[]):\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "    best_vloss = 1_000_000\n",
    "\n",
    "    for epoch in range(n_epochs):    \n",
    "        model.train(True)\n",
    "        \n",
    "        avg_loss = train_one_epoch(epoch, writer, n_epochs)\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        model.train(False)\n",
    "        with torch.no_grad():\n",
    "            running_vloss = 0.0\n",
    "            \n",
    "            for i, vdata in enumerate(val_loader):\n",
    "                \n",
    "                local_X, local_y = vdata\n",
    "                local_X, local_y = local_X.to(device), local_y.to(device)\n",
    "                            \n",
    "                voutputs = model(local_X)\n",
    "                \n",
    "                B, T, C = voutputs.shape\n",
    "                voutputs = voutputs.view(B*T, C)\n",
    "                local_y = local_y.view(B*T)\n",
    "                \n",
    "                vloss = criterion(voutputs, local_y)\n",
    "                running_vloss += vloss\n",
    "                \n",
    "            avg_vloss = running_vloss / (i+1)\n",
    "            val_losses.append(vloss)\n",
    "            \n",
    "            print(\n",
    "                f'Epoch [{epoch+1}/{n_epochs}], Train-Loss: {avg_loss:.4f}, Val-Loss: {avg_vloss:.4f}')\n",
    "            \n",
    "            writer.add_scalars('Training vs. Validation Loss', {'Training': avg_loss, 'Validation': avg_vloss}, epoch)\n",
    "            writer.flush()\n",
    "            \n",
    "            if not respect_val or (respect_val and avg_vloss < best_vloss):\n",
    "                best_vloss = avg_vloss\n",
    "                model_path = './models/_model_{}_{}'.format(timestamp, epoch)\n",
    "                print(f'save new model: {model_path}')\n",
    "                torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_losses = []\n",
    "train_losses = []\n",
    "train(10, respect_val=True, val_losses=val_losses, train_losses=train_losses)\n",
    "val_losses = list(map(lambda x : x.item(), val_losses))\n",
    "train_losses = list(map(lambda x : x.item(), train_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(1, len(val_losses)+1, 1), val_losses, label=\"val\")\n",
    "plt.plot(np.arange(1, len(train_losses)+1, 1), train_losses, label=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'there are the following models to choose from:')\n",
    "\n",
    "for model_file in os.listdir('./models/'):\n",
    "    print(f'./models/{model_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads a saved model\n",
    "model_path = './models/pretrained_32_2_2'\n",
    "\n",
    "if device.type == 'cpu':\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Melodiegenerierung\n",
    "\n",
    "Gegeben einer Sequenz beliebiger Länge, dient die Funktion ``generate`` der Generierung eines neues neuen Musikstücks.\n",
    "``temperature`` bestimmt wie stark die vom Modell gelernte Wahrscheinlichkeitsverteilung beachtet wird.\n",
    "\n",
    "+ ``temperature`` gleich 1.0 bedeutet, dass von der Wahrscheinlichkeitsverteilung gesampelt wird.\n",
    "+ ``temperature`` gegen unendlich bedeutet, dass gleichverteilt gesampelt wird (mehr Variation)\n",
    "+ ``temperature`` gegen 0 bedeutet, dass die hohe Wahrscheinlichkeiten verstärkt werden (weniger Variation)\n",
    "\n",
    "Sie können eine maximale Länge des Stücks festlegen und auch einen Anfang eines Stücks mitliefern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_event_number(idx, temperature:float):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(idx[:,-sequence_len:])\n",
    "        B, T, C = outputs.shape\n",
    "        logits = outputs[:, -1, :]\n",
    "        probs = F.softmax(logits / temperature, dim=1)  # B, C\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        return idx_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(seq: list[str]=None, max_len:int=None, temperature:float=1.0):\n",
    "    with torch.no_grad():\n",
    "        generated_encoded_song = []\n",
    "        start_sequence = [string_to_int.encode(TERM_SYMBOL)]*sequence_len\n",
    "        if seq != None:\n",
    "            start_sequence = start_sequence + [string_to_int.encode(char) for char in seq]\n",
    "            idx = torch.tensor([start_sequence], device=device)\n",
    "            generated_encoded_song = seq.copy()\n",
    "        else:\n",
    "            idx = torch.tensor([start_sequence], device=device)\n",
    "        \n",
    "        while max_len == None or max_len > len(generated_encoded_song):\n",
    "            idx_next = next_event_number(idx, temperature)\n",
    "            char = string_to_int.decode(idx_next.item())\n",
    "            if idx_next == string_to_int.encode(TERM_SYMBOL):\n",
    "                break\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # B, T+1, C\n",
    "            generated_encoded_song.append(char)\n",
    "            \n",
    "        return generated_encoded_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of songs we want to generate\n",
    "n_scores = 5\n",
    "temperature = 0.6\n",
    "after_new_songs = []\n",
    "for _ in range(n_scores):\n",
    "    encoded_song = generate(max_len=120,temperature=temperature)\n",
    "    print(f'generated {\" \".join(encoded_song)} conisting of {len(encoded_song)} notes')\n",
    "    after_new_songs.append(encoded_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_generated_scores = encoder.decode_songs(after_new_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_generated_scores[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(score_to_wav(after_generated_scores[0], 'a_g_song.wav'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können auch einen Teil bestehendes Musikstücks verwenden und diesen erweitern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(enc_songs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_notes = 10\n",
    "part = encoder.take_notes(enc_songs[0], n_notes)\n",
    "' '.join(part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Audio(score_to_wav(encoder.decode_song(part), 'part.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_song = generate(part, max_len=120,temperature=temperature)\n",
    "' '.join(enc_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song = encoder.decode_song(enc_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(score_to_wav(song, 'g_song.wav'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fragen\n",
    "\n",
    "+ Welche Unterschiede zwischen LSTM und Transformer kennen Sie?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
